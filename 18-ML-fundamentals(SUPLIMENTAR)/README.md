# SÄƒptÄƒmÃ¢na 18: Fundamente Machine Learning Ã®n C

## ğŸ¯ Obiective de ÃnvÄƒÈ›are

La finalul acestei sÄƒptÄƒmÃ¢ni, veÈ›i fi capabili sÄƒ:

1. **ReÈ›ineÈ›i** (Remember): Formulele fundamentale pentru MSE È™i gradient Ã®n regresie liniarÄƒ; regula de actualizare a centroizilor Ã®n K-Means; distanÈ›a EuclidianÄƒ pentru K-NN; regula de actualizare a ponderilor pentru Perceptron
2. **ÃnÈ›elegeÈ›i** (Understand): Mecanismul convergenÈ›ei Ã®n gradient descent È™i factorii care o influenÈ›eazÄƒ; efectul parametrului k asupra calitÄƒÈ›ii clustering-ului; limitÄƒrile perceptronului pentru probleme neliniar separabile (XOR)
3. **AplicaÈ›i** (Apply): Implementarea completÄƒ Ã®n C a: regresiei liniare cu gradient descent, K-Means cu iniÈ›ializare k-means++, clasificatorului K-NN, È™i perceptronului cu regulÄƒ de Ã®nvÄƒÈ›are
4. **AnalizaÈ›i** (Analyse): Interpretarea curbelor de convergenÈ›Äƒ (loss curve); evaluarea calitÄƒÈ›ii clustering-ului prin inerÈ›ie È™i silhouette simplificat; analiza matricei de confuzie
5. **EvaluaÈ›i** (Evaluate): Selectarea hiperparametrilor optimi (learning rate, k, numÄƒr de epoci) È™i evaluarea trade-off-urilor asociate
6. **CreaÈ›i** (Create): Sistem complet de clasificare multi-clasÄƒ pentru setul de date Iris cu train/test split È™i evaluare riguroasÄƒ

---

## ğŸ“œ Context Istoric

### Originile ÃnvÄƒÈ›Äƒrii Automate

ÃnvÄƒÈ›area automatÄƒ (Machine Learning) Ã®È™i are rÄƒdÄƒcinile Ã®n visul uman de a crea maÈ™ini capabile sÄƒ Ã®nveÈ›e din experienÈ›Äƒ. AceastÄƒ disciplinÄƒ a evoluat la intersecÈ›ia statisticii, informaticii È™i neuroÈ™tiinÈ›elor, transformÃ¢ndu-se dintr-o curiozitate academicÄƒ Ã®ntr-o tehnologie care redefineÈ™te societatea modernÄƒ.

Conceptul de "maÈ™inÄƒ care Ã®nvaÈ›Äƒ" a fost articulat pentru prima datÄƒ de **Alan Turing** Ã®n articolul sÄƒu revoluÈ›ionar din 1950, *"Computing Machinery and Intelligence"*, unde a propus celebrul test care Ã®i poartÄƒ numele. Turing a imaginat cÄƒ, Ã®n loc sÄƒ programÄƒm explicit fiecare comportament, am putea construi sisteme care sÄƒ Ã®nveÈ›e din date, similar modului Ã®n care copiii Ã®nvaÈ›Äƒ din experienÈ›Äƒ.

### Figuri Cheie

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Frank Rosenblatt (1928-1971)                                   â”‚
    â”‚  Psiholog È™i informatician american                             â”‚
    â”‚                                                                 â”‚
    â”‚  â€¢ Inventatorul Perceptronului (1958) la Cornell                â”‚
    â”‚  â€¢ A construit Mark I Perceptron - primul hardware neuronal     â”‚
    â”‚  â€¢ Pionierul reÈ›elelor neuronale artificiale                    â”‚
    â”‚  â€¢ "Father of Deep Learning" - recunoscut postum                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> *"The embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."*
> â€” The New York Times, despre Perceptron, 1958

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Stuart Lloyd (1923-2010)                                       â”‚
    â”‚  Matematician la Bell Labs                                      â”‚
    â”‚                                                                 â”‚
    â”‚  â€¢ A dezvoltat algoritmul K-Means Ã®n 1957                       â”‚
    â”‚  â€¢ Publicat oficial abia Ã®n 1982 Ã®n IEEE                        â”‚
    â”‚  â€¢ Algoritmul e cunoscut È™i ca "Lloyd's algorithm"              â”‚
    â”‚  â€¢ AplicaÈ›ii Ã®n compresia datelor È™i cuantizare vectorialÄƒ      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Adrien-Marie Legendre (1752-1833) & Carl Friedrich Gauss       â”‚
    â”‚  (1777-1855)                                                    â”‚
    â”‚  Matematicieni francez È™i german                                â”‚
    â”‚                                                                 â”‚
    â”‚  â€¢ Legendre: Prima publicare a metodei celor mai mici pÄƒtrate   â”‚
    â”‚    Ã®n 1805 ("Nouvelles mÃ©thodes pour la dÃ©termination des       â”‚
    â”‚    orbites des comÃ¨tes")                                        â”‚
    â”‚  â€¢ Gauss: A revendicat utilizarea metodei Ã®ncÄƒ din 1795         â”‚
    â”‚  â€¢ Fundamentul matematic al regresiei liniare moderne           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Moment Crucial: Perceptrons (1969)

Cartea *"Perceptrons: An Introduction to Computational Geometry"* de **Marvin Minsky** È™i **Seymour Papert** a demonstrat matematic cÄƒ perceptronul simplu nu poate Ã®nvÄƒÈ›a funcÈ›ia XOR. AceastÄƒ demonstraÈ›ie a declanÈ™at primul "AI Winter" - o perioadÄƒ de reducere drasticÄƒ a finanÈ›Äƒrii cercetÄƒrii Ã®n reÈ›ele neuronale. Ironic, soluÈ›ia (reÈ›elele multi-strat cu backpropagation) exista deja Ã®n formÄƒ incipientÄƒ, dar a fost nevoie de douÄƒ decenii pentru redescoperire.

---

## ğŸ“š Fundamente Teoretice

### 1. InfrastructurÄƒ MatricealÄƒ pentru ML

Algoritmii ML opereazÄƒ fundamental pe vectori È™i matrice. Ãn C, trebuie sÄƒ implementÄƒm aceste abstracÈ›ii de la zero:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    REPREZENTAREA DATELOR                        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚   Dataset Iris (exemplu):                                       â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚ Feature 1 â”‚ Feature 2 â”‚ Feature 3 â”‚ Feature 4 â”‚ Label   â”‚   â”‚
    â”‚   â”‚ (sepal L) â”‚ (sepal W) â”‚ (petal L) â”‚ (petal W) â”‚         â”‚   â”‚
    â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
    â”‚   â”‚   5.1     â”‚   3.5     â”‚   1.4     â”‚   0.2     â”‚    0    â”‚   â”‚
    â”‚   â”‚   7.0     â”‚   3.2     â”‚   4.7     â”‚   1.4     â”‚    1    â”‚   â”‚
    â”‚   â”‚   6.3     â”‚   3.3     â”‚   6.0     â”‚   2.5     â”‚    2    â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                 â”‚
    â”‚   Ãn C: double **X  (n_samples Ã— n_features)                    â”‚
    â”‚         int *y      (n_samples)                                 â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Structurile fundamentale Ã®n C:**

```c
typedef struct {
    double *data;       /* Date stocate liniar (row-major) */
    size_t rows;        /* NumÄƒr de linii */
    size_t cols;        /* NumÄƒr de coloane */
} Matrix;

typedef struct {
    double *data;       /* Elementele vectorului */
    size_t size;        /* Dimensiunea vectorului */
} Vector;
```

### 2. Regresie LiniarÄƒ cu Gradient Descent

Regresia liniarÄƒ modeleazÄƒ relaÈ›ia liniarÄƒ dintre features (X) È™i target (y):

```
    Å· = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b = wÂ·x + b
    
    unde:
    â€¢ w = vector de ponderi (weights)
    â€¢ b = bias (intercept)
    â€¢ Å· = predicÈ›ia modelului
```

**FuncÈ›ia de cost (Mean Squared Error):**

```
                    1   n
    MSE(w, b) = â”€â”€â”€â”€â”€  Î£  (Å·áµ¢ - yáµ¢)Â²
                  n   i=1
```

**Gradient Descent:**

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    GRADIENT DESCENT                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚   IntuiÈ›ie: CoborÃ¢m pe "suprafaÈ›a" funcÈ›iei de cost             â”‚
    â”‚                                                                 â”‚
    â”‚   Loss                                                          â”‚
    â”‚     â”‚     â•­â”€â”€â”€â•®                                                 â”‚
    â”‚     â”‚    â•±     â•²                                                â”‚
    â”‚     â”‚   â•±   â—   â•²       â— = poziÈ›ia curentÄƒ                     â”‚
    â”‚     â”‚  â•±     â†“   â•²      â†“ = direcÈ›ia gradientului               â”‚
    â”‚     â”‚ â•±       â—   â•²                                             â”‚
    â”‚     â”‚â•±         â†“   â•²                                            â”‚
    â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
    â”‚               (minim)      w                                    â”‚
    â”‚                                                                 â”‚
    â”‚   Regula de actualizare:                                        â”‚
    â”‚   â€¢ w = w - Î± Â· âˆ‚MSE/âˆ‚w                                         â”‚
    â”‚   â€¢ b = b - Î± Â· âˆ‚MSE/âˆ‚b                                         â”‚
    â”‚                                                                 â”‚
    â”‚   unde Î± = learning rate (ratÄƒ de Ã®nvÄƒÈ›are)                     â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Derivatele:**

```
    âˆ‚MSE          2   n
    â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€  Î£  (Å·áµ¢ - yáµ¢) Â· xáµ¢â±¼
    âˆ‚wâ±¼       n   i=1

    âˆ‚MSE          2   n
    â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€  Î£  (Å·áµ¢ - yáµ¢)
    âˆ‚b        n   i=1
```

### 3. K-Means Clustering

K-Means partiÈ›ioneazÄƒ datele Ã®n k clustere minimizÃ¢nd inerÈ›ia (within-cluster sum of squares):

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    ALGORITMUL K-MEANS                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚   1. IniÈ›ializare: Alege k centroizi                            â”‚
    â”‚                                                                 â”‚
    â”‚   2. RepetÄƒ pÃ¢nÄƒ la convergenÈ›Äƒ:                                â”‚
    â”‚      a) Asignare: Fiecare punct â†’ cel mai apropiat centroid     â”‚
    â”‚      b) Actualizare: Centroid = media punctelor asignate        â”‚
    â”‚                                                                 â”‚
    â”‚   Vizualizare (k=3):                                            â”‚
    â”‚                                                                 â”‚
    â”‚       IteraÈ›ia 1:        IteraÈ›ia 2:        ConvergenÈ›Äƒ:        â”‚
    â”‚       â—‹ â—‹                â—‹ â—‹                â— â—                 â”‚
    â”‚      â—‹   Ã—              â—   Ã—              â—   Ã—                â”‚
    â”‚        â—‹                  â—                  â—                  â”‚
    â”‚                                                                 â”‚
    â”‚      â–¡ â–¡ â–¡              â–¡ â–¡ â–¡              â–  â–  â–                 â”‚
    â”‚         Ã—                  Ã—                  Ã—                 â”‚
    â”‚      â–¡ â–¡                â–  â–                 â–  â–                   â”‚
    â”‚                                                                 â”‚
    â”‚      â–³   â–³              â–³   â–³              â–²   â–²                â”‚
    â”‚        Ã— â–³                Ã— â–²                Ã— â–²                â”‚
    â”‚      â–³                  â–²                  â–²                    â”‚
    â”‚                                                                 â”‚
    â”‚   Ã— = centroid, forme goale = neasignat, pline = asignat        â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**K-Means++ Initialization:**

IniÈ›ializarea aleatoare poate duce la rezultate suboptimale. K-means++ Ã®mbunÄƒtÄƒÈ›eÈ™te dramatic convergenÈ›a:

```
    1. Alege primul centroid uniform aleator din date
    2. Pentru fiecare centroid urmÄƒtor:
       a) CalculeazÄƒ D(x)Â² = distanÈ›a minimÄƒ de la x la centroizii existenÈ›i
       b) Alege noul centroid cu probabilitate âˆ D(x)Â²
    3. RepetÄƒ pÃ¢nÄƒ avem k centroizi
```

### 4. K-Nearest Neighbors (K-NN)

K-NN este un algoritm "lazy" - nu construieÈ™te un model explicit, ci memoreazÄƒ toate datele de antrenament:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    CLASIFICARE K-NN                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚   Pentru un punct nou ?:                                        â”‚
    â”‚                                                                 â”‚
    â”‚   k=3:                        k=5:                              â”‚
    â”‚   â—‹ â—‹                         â—‹ â—‹                               â”‚
    â”‚     â—‹                           â—‹    â—‹                          â”‚
    â”‚       ?  â—‹                        ?  â—‹                          â”‚
    â”‚     â–¡                           â–¡                               â”‚
    â”‚   â–¡   â–¡                       â–¡   â–¡                             â”‚
    â”‚                                                                 â”‚
    â”‚   Vecini: â—‹â—‹â—‹ â†’ Clasa: â—‹      Vecini: â—‹â—‹â—‹â–¡â–¡ â†’ Clasa: â—‹          â”‚
    â”‚   (2â—‹, 1â–¡) â†’ majoritate â—‹     (3â—‹, 2â–¡) â†’ majoritate â—‹           â”‚
    â”‚                                                                 â”‚
    â”‚   DistanÈ›a EuclidianÄƒ:                                          â”‚
    â”‚                   ___________________________                   â”‚
    â”‚   d(a,b) = âˆš Î£áµ¢ (aáµ¢ - báµ¢)Â²                                      â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Perceptronul

Perceptronul este cel mai simplu model de neuron artificial, capabil sÄƒ Ã®nveÈ›e graniÈ›e de decizie liniare:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    STRUCTURA PERCEPTRONULUI                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚                    xâ‚ â”€â”€â”€â”€â”€â”                                    â”‚
    â”‚                            â”‚wâ‚                                  â”‚
    â”‚                    xâ‚‚ â”€â”€â”€â”€â”€â”¼â”€â”€â†’ Î£ â”€â”€â†’ f(Â·) â”€â”€â†’ Å·               â”‚
    â”‚                            â”‚wâ‚‚       â†‘                          â”‚
    â”‚                    xâ‚ƒ â”€â”€â”€â”€â”€â”˜         â”‚                          â”‚
    â”‚                        wâ‚ƒ    b â”€â”€â”€â”€â”€â”€â”˜                          â”‚
    â”‚                                                                 â”‚
    â”‚   Suma ponderatÄƒ: z = wÂ·x + b = Î£áµ¢ wáµ¢xáµ¢ + b                     â”‚
    â”‚   FuncÈ›ia de activare: f(z) = sign(z) = { +1 dacÄƒ z â‰¥ 0        â”‚
    â”‚                                         { -1 dacÄƒ z < 0         â”‚
    â”‚                                                                 â”‚
    â”‚   Regula de Ã®nvÄƒÈ›are (Perceptron Learning Rule):                â”‚
    â”‚   DacÄƒ yáµ¢ Â· (wÂ·xáµ¢ + b) â‰¤ 0 (clasificare greÈ™itÄƒ):              â”‚
    â”‚       w = w + Î± Â· yáµ¢ Â· xáµ¢                                       â”‚
    â”‚       b = b + Î± Â· yáµ¢                                            â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Limitarea XOR:**

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    PROBLEMA XOR                                 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                 â”‚
    â”‚   AND (separabil liniar):    XOR (NU e separabil liniar):       â”‚
    â”‚                                                                 â”‚
    â”‚   yâ”‚                         yâ”‚                                 â”‚
    â”‚   1â”‚    â—‹ (1,1)=1            1â”‚    â— (0,1)=1    â—‹ (1,1)=0       â”‚
    â”‚    â”‚      â•²                   â”‚                                 â”‚
    â”‚    â”‚       â•²                  â”‚         ???                     â”‚
    â”‚   0â”‚â—‹ â—‹     â•²                0â”‚    â—‹ (0,0)=0    â— (1,0)=1       â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€x             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€x            â”‚
    â”‚      0    1                     0         1                     â”‚
    â”‚                                                                 â”‚
    â”‚   O singurÄƒ linie poate       NU existÄƒ o linie care sÄƒ         â”‚
    â”‚   separa clasa 1 de clasa 0   separe clasele 0 È™i 1!            â”‚
    â”‚                                                                 â”‚
    â”‚   SoluÈ›ia: Multi-Layer Perceptron (MLP) cu layer ascuns         â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ AplicaÈ›ii Industriale

### 1. Regresie LiniarÄƒ Ã®n PracticÄƒ

- **PredicÈ›ia preÈ›urilor imobiliare**: Estimarea valorii proprietÄƒÈ›ilor bazatÄƒ pe features (suprafaÈ›Äƒ, locaÈ›ie, numÄƒr camere)
- **Forecasting vÃ¢nzÄƒri**: ProiecÈ›ii bazate pe date istorice È™i factori sezonieri
- **Analiza riscului de credit**: Scoring bazat pe caracteristici financiare

### 2. K-Means Ã®n Industrie

- **Segmentarea clienÈ›ilor**: Gruparea bazatÄƒ pe comportament de cumpÄƒrare (RFM: Recency, Frequency, Monetary)
- **Compresia imaginilor**: Reducerea numÄƒrului de culori prin cuantizare
- **Detectarea anomaliilor**: Identificarea outlier-ilor ca puncte departe de orice centroid

### 3. K-NN Ã®n AplicaÈ›ii Reale

- **Sisteme de recomandare**: "Users like you also purchased..."
- **RecunoaÈ™terea scrisului de mÃ¢nÄƒ**: MNIST digit classification
- **Diagnosticare medicalÄƒ**: Clasificare bazatÄƒ pe cazuri similare anterioare

### 4. Perceptron È™i DescendenÈ›ii SÄƒi

- **Spam filtering**: Clasificarea email-urilor (origini istorice)
- **Clasificare de sentiment**: Pozitiv/Negativ Ã®n recenzii
- **Deep Learning**: ReÈ›elele neuronale moderne sunt extensii ale perceptronului

---

## ğŸ’» ExerciÈ›ii de Laborator

### ExerciÈ›iul 1: Customer Segmentation cu K-Means

**Obiectiv**: ImplementaÈ›i segmentarea clienÈ›ilor folosind K-Means cu k-means++ initialization.

**CerinÈ›e**:
1. CitiÈ›i datele clienÈ›ilor din `data/customers.csv` (venit anual, frecvenÈ›Äƒ achiziÈ›ii, valoare totalÄƒ)
2. NormalizaÈ›i datele folosind Min-Max scaling
3. ImplementaÈ›i K-Means cu iniÈ›ializare k-means++
4. AplicaÈ›i metoda Elbow pentru a determina k optim
5. AfiÈ™aÈ›i profilul fiecÄƒrui cluster identificat

**FiÈ™iere**: `src/exercise1.c`, `data/customers.csv`

### ExerciÈ›iul 2: Multi-class Classification cu Perceptron

**Obiectiv**: ImplementaÈ›i clasificarea multi-clasÄƒ pentru setul Iris folosind strategia One-vs-All.

**CerinÈ›e**:
1. CitiÈ›i datele din `data/iris.csv`
2. ImplementaÈ›i train/test split (80/20)
3. AntrenaÈ›i 3 perceptroni (unul pentru fiecare clasÄƒ)
4. ImplementaÈ›i predicÈ›ia combinatÄƒ (max score)
5. CalculaÈ›i È™i afiÈ™aÈ›i accuracy È™i matricea de confuzie

**FiÈ™iere**: `src/exercise2.c`, `data/iris.csv`

---

## ğŸ”§ Compilare È™i ExecuÈ›ie

```bash
# ConstruieÈ™te toate targeturile
make all

# RuleazÄƒ exemplul demonstrativ
make run

# CompileazÄƒ È™i ruleazÄƒ exerciÈ›iul 1
make exercise1
./exercise1

# CompileazÄƒ È™i ruleazÄƒ exerciÈ›iul 2
make exercise2
./exercise2

# RuleazÄƒ testele automate
make test

# Verificare memory leaks
make valgrind

# CurÄƒÈ›are
make clean

# Ajutor
make help
```

---

## ğŸ³ Rulare Ã®n Docker

```bash
# ConstruieÈ™te imaginea
docker build -t atp-week18 .

# RuleazÄƒ containerul
docker run -it --rm -v $(pwd):/workspace atp-week18

# Ãn container:
make all && make run
```

---

## ğŸ“ Structura Directorului

```
18-ml-fundamentals-c/
â”œâ”€â”€ README.md                           # DocumentaÈ›ia completÄƒ (acest fiÈ™ier)
â”œâ”€â”€ Makefile                            # Build system cu output colorat
â”œâ”€â”€ Dockerfile                          # Containerizare pentru portabilitate
â”œâ”€â”€ docker-compose.yml                  # Setup complet cu toate dependenÈ›ele
â”‚
â”œâ”€â”€ slides/
â”‚   â”œâ”€â”€ presentation-week18.html        # Prezentare interactivÄƒ principalÄƒ
â”‚   â””â”€â”€ presentation-comparativ.html    # ComparaÈ›ie Pseudocod/C/Python
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ example1.c                      # Demo complet: LR, K-Means, K-NN, Perceptron
â”‚   â”œâ”€â”€ exercise1.c                     # Customer Segmentation (cu TODO-uri)
â”‚   â””â”€â”€ exercise2.c                     # Multi-class Classification (cu TODO-uri)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ iris.csv                        # Dataset Iris clasic (150 samples)
â”‚   â”œâ”€â”€ housing_simple.csv              # Date pentru regresie
â”‚   â”œâ”€â”€ customers.csv                   # Date pentru clustering
â”‚   â””â”€â”€ xor_data.csv                    # Date XOR pentru demo limitÄƒri
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test1_input.txt                 # Input pentru test clustering
â”‚   â”œâ”€â”€ test1_expected.txt              # Output aÈ™teptat clustering
â”‚   â”œâ”€â”€ test2_input.txt                 # Input pentru test clasificare
â”‚   â””â”€â”€ test2_expected.txt              # Output aÈ™teptat clasificare
â”‚
â”œâ”€â”€ python_comparison/
â”‚   â”œâ”€â”€ sklearn_comparison.py           # Echivalent scikit-learn
â”‚   â””â”€â”€ numpy_operations.py             # OperaÈ›ii matriceale Ã®n NumPy
â”‚
â”œâ”€â”€ teme/
â”‚   â”œâ”€â”€ homework-requirements.md        # CerinÈ›e temÄƒ (Ridge Regression, Weighted K-NN)
â”‚   â””â”€â”€ homework-extended.md            # ProvocÄƒri avansate (5)
â”‚
â””â”€â”€ solution/
    â”œâ”€â”€ exercise1_sol.c                 # SoluÈ›ie completÄƒ Ex. 1
    â”œâ”€â”€ exercise2_sol.c                 # SoluÈ›ie completÄƒ Ex. 2
    â”œâ”€â”€ homework1_sol.c                 # SoluÈ›ie Tema 1
    â””â”€â”€ homework2_sol.c                 # SoluÈ›ie Tema 2
```

---

## ğŸ“– Bibliografie RecomandatÄƒ

### EsenÈ›iale

- **Tom M. Mitchell** - *Machine Learning* (1997), Capitolele 1-4
- **Christopher M. Bishop** - *Pattern Recognition and Machine Learning* (2006), Capitolul 1
- **Gareth James et al.** - *An Introduction to Statistical Learning* (2013), Capitolele 2-3

### Avansate

- **Marvin Minsky & Seymour Papert** - *Perceptrons: An Introduction to Computational Geometry* (1969) - Document istoric fundamental
- **Stuart Lloyd** - *Least Squares Quantization in PCM* (1982), IEEE Transactions on Information Theory

### Resurse Online

- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html) - ReferinÈ›Äƒ pentru implementÄƒri profesionale
- [Stanford CS229: Machine Learning](https://cs229.stanford.edu/) - Curs universitar comprehensiv
- [3Blue1Brown: Neural Networks](https://www.3blue1brown.com/topics/neural-networks) - VizualizÄƒri excelente

---

## âœ… Lista de Auto-Verificare

### ÃnÈ›elegere TeoreticÄƒ
- [ ] Pot explica diferenÈ›a dintre supervised È™i unsupervised learning
- [ ] ÃnÈ›eleg de ce normalizarea datelor este importantÄƒ
- [ ] Pot deriva manual gradientul pentru MSE
- [ ] ÃnÈ›eleg de ce perceptronul nu poate rezolva XOR

### Implementare PracticÄƒ
- [ ] Am implementat regresie liniarÄƒ cu gradient descent
- [ ] Modelul meu converge (loss scade Ã®n timp)
- [ ] Am implementat K-Means cu k-means++ initialization
- [ ] K-NN clasificÄƒ corect pe date de test
- [ ] Perceptronul converge pe date liniar separabile

### Evaluare È™i Debugging
- [ ] È˜tiu sÄƒ interpretez curba de loss
- [ ] Pot identifica overfitting vs. underfitting
- [ ] È˜tiu sÄƒ aleg k optim pentru K-Means (metoda Elbow)
- [ ] ÃnÈ›eleg matricea de confuzie

---

## ğŸ’¼ PregÄƒtire pentru Interviu

### ÃntrebÄƒri Frecvente

1. **Care este diferenÈ›a dintre regresie È™i clasificare?**
   - Regresia prezice valori continue (ex: preÈ›), clasificarea prezice categorii discrete (ex: spam/not spam)

2. **De ce normalizÄƒm datele Ã®nainte de K-Means?**
   - Features cu scale diferite dominÄƒ distanÈ›a EuclidianÄƒ; normalizarea asigurÄƒ contribuÈ›ie egalÄƒ

3. **Care e complexitatea K-NN la inference?**
   - O(nÂ·d) pentru fiecare predicÈ›ie, unde n = numÄƒr samples, d = dimensiuni; de aceea e "lazy" È™i ineficient pentru date mari

4. **De ce perceptronul nu poate Ã®nvÄƒÈ›a XOR?**
   - XOR nu e liniar separabil; o singurÄƒ hiperplan nu poate separa clasele

5. **CÃ¢nd foloseÈ™ti K-Means vs. Hierarchical Clustering?**
   - K-Means: k cunoscut, date mari, clustere sferice; Hierarchical: k necunoscut, dendrogramÄƒ utilÄƒ, date mici-medii

---

## ğŸ”— Preview SÄƒptÄƒmÃ¢na UrmÄƒtoare

**SÄƒptÄƒmÃ¢na 19: Algoritmi pentru IoT È™i Stream Processing**

Vom explora algoritmi optimizaÈ›i pentru dispozitive cu resurse limitate È™i procesarea fluxurilor de date Ã®n timp real:

- Filtre digitale: Moving Average, Exponential Moving Average, Kalman 1D
- Ferestre temporale: Tumbling, Sliding, Session
- Detectarea anomaliilor Ã®n stream-uri de date
- Simulare senzori IoT Ã®n Docker

LegÄƒtura cu ML: Algoritmii de streaming pot incorpora modele ML pentru predicÈ›ii Ã®n timp real (online learning).

---

## ğŸ”§ Hardware Real (OpÈ›ional)

Kit-uri Arduino 32-bit disponibile la Biblioteca FacultÄƒÈ›ii pentru studenÈ›ii interesaÈ›i de ML pe edge devices:

- **ESP32 DevKit** - Dual-core, WiFi/BLE, suficient pentru inference pe modele mici
- **Arduino Due** - ARM Cortex-M3, mai multÄƒ memorie pentru modele complexe

**Avantaje hardware real:**
- ÃnÈ›elegerea constrÃ¢ngerilor de memorie È™i compute
- ExperienÈ›a cu fixed-point arithmetic (Ã®n loc de float)
- AplicaÈ›ii IoT practice cu ML la edge

**NotÄƒ**: Toate exerciÈ›iile sunt 100% realizabile Ã®n Docker pe laptop standard. Hardware-ul este doar pentru explorare suplimentarÄƒ.

---

*Acest material face parte din cursul "Algoritmi È™i Tehnici de Programare" (ATP)*
*Academia de Studii Economice din BucureÈ™ti - CSIE*
*Actualizat: Ianuarie 2026*

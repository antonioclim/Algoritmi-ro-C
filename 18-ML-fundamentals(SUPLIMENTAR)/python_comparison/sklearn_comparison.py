#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
COMPARAÈšIE: ImplementÄƒri C vs scikit-learn
SÄƒptÄƒmÃ¢na 18: Fundamente Machine Learning Ã®n C
================================================================================

Acest script demonstreazÄƒ echivalenÈ›a dintre implementÄƒrile noastre Ã®n C
È™i bibliotecile profesionale din Python (scikit-learn).

Scopul este EDUCAÈšIONAL:
- ValidÄƒm cÄƒ algoritmii noÈ™tri produc rezultate similare
- ObservÄƒm diferenÈ›ele de performanÈ›Äƒ (timp de execuÈ›ie)
- ÃnÈ›elegem API-ul profesional pentru viitoarele proiecte

Autor: Curs ATP - ASE BucureÈ™ti
================================================================================
"""

import numpy as np
from sklearn.linear_model import LinearRegression, Perceptron
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, r2_score,
    accuracy_score, classification_report,
    silhouette_score
)
import time
import pandas as pd

# ==============================================================================
# PARTEA 1: REGRESIE LINIARÄ‚
# ==============================================================================

def demo_linear_regression():
    """
    ComparÄƒ regresia liniarÄƒ implementatÄƒ Ã®n C cu sklearn.
    
    Ãn C: Gradient Descent iterativ
    Ãn sklearn: EcuaÈ›ia normalÄƒ (closed-form) sau SVD
    """
    print("\n" + "="*70)
    print("PARTEA 1: REGRESIE LINIARÄ‚")
    print("="*70)
    
    # ÃncarcÄƒ datele
    try:
        df = pd.read_csv('data/housing_simple.csv', comment='#')
    except FileNotFoundError:
        print("EROARE: FiÈ™ierul data/housing_simple.csv nu a fost gÄƒsit!")
        return
    
    # PregÄƒteÈ™te datele
    X = df[['sqft', 'bedrooms', 'bathrooms']].values
    y = df['price'].values
    
    # Normalizare (la fel ca Ã®n implementarea C)
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # Antrenare sklearn
    start_time = time.time()
    model = LinearRegression()
    model.fit(X_train, y_train)
    sklearn_time = time.time() - start_time
    
    # PredicÈ›ii
    y_pred = model.predict(X_test)
    
    # Metrici
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"\nğŸ“Š Rezultate sklearn LinearRegression:")
    print(f"   â€¢ Timp antrenare: {sklearn_time*1000:.3f} ms")
    print(f"   â€¢ MSE (test): {mse:.4f}")
    print(f"   â€¢ RÂ² score: {r2:.4f}")
    print(f"\n   CoeficienÈ›i (weights):")
    print(f"   â€¢ w_sqft:     {model.coef_[0]:.4f}")
    print(f"   â€¢ w_bedrooms: {model.coef_[1]:.4f}")
    print(f"   â€¢ w_bath:     {model.coef_[2]:.4f}")
    print(f"   â€¢ bias:       {model.intercept_:.4f}")
    
    print("\nğŸ’¡ DiferenÈ›a faÈ›Äƒ de implementarea C:")
    print("   â€¢ sklearn foloseÈ™te SVD (Singular Value Decomposition)")
    print("   â€¢ SoluÈ›ie exactÄƒ Ã®n O(nÂ·dÂ²), nu iterativÄƒ")
    print("   â€¢ Gradient Descent Ã®n C: mai lent, dar educaÈ›ional")


# ==============================================================================
# PARTEA 2: K-MEANS CLUSTERING
# ==============================================================================

def demo_kmeans():
    """
    ComparÄƒ K-Means implementat Ã®n C cu sklearn.
    
    Ambele folosesc Lloyd's Algorithm, dar sklearn are:
    - K-Means++ initialization (default)
    - Paralelizare pe mai multe nuclee
    - OptimizÄƒri Elkan
    """
    print("\n" + "="*70)
    print("PARTEA 2: K-MEANS CLUSTERING")
    print("="*70)
    
    # ÃncarcÄƒ datele
    try:
        df = pd.read_csv('data/customers.csv', comment='#')
    except FileNotFoundError:
        print("EROARE: FiÈ™ierul data/customers.csv nu a fost gÄƒsit!")
        return
    
    # PregÄƒteÈ™te datele
    X = df[['annual_income', 'spending_score']].values
    
    # Normalizare
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    # TesteazÄƒ diferite valori de k (Elbow Method)
    print("\nğŸ“Š Elbow Method - Inertia pentru diferite k:")
    print("-" * 40)
    
    inertias = []
    silhouettes = []
    
    for k in range(2, 8):
        kmeans = KMeans(n_clusters=k, init='k-means++', 
                       n_init=10, random_state=42)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
        sil = silhouette_score(X_scaled, kmeans.labels_)
        silhouettes.append(sil)
        print(f"   k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil:.4f}")
    
    # K optim (Ã®n acest caz, k=4)
    optimal_k = 4
    print(f"\n   â¤ K optim selectat: {optimal_k}")
    
    # Antrenare finalÄƒ
    start_time = time.time()
    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', 
                   n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    sklearn_time = time.time() - start_time
    
    print(f"\nğŸ“Š Rezultate sklearn KMeans (k={optimal_k}):")
    print(f"   â€¢ Timp antrenare: {sklearn_time*1000:.3f} ms")
    print(f"   â€¢ Inertia finalÄƒ: {kmeans.inertia_:.4f}")
    print(f"   â€¢ IteraÈ›ii pÃ¢nÄƒ la convergenÈ›Äƒ: {kmeans.n_iter_}")
    
    print(f"\n   Centroizi (normalizaÈ›i):")
    for i, centroid in enumerate(kmeans.cluster_centers_):
        count = np.sum(kmeans.labels_ == i)
        print(f"   â€¢ Cluster {i}: ({centroid[0]:.3f}, {centroid[1]:.3f}) - {count} clienÈ›i")
    
    # Profilare clustere
    df['cluster'] = kmeans.labels_
    print(f"\n   Profilare clustere (date originale):")
    for i in range(optimal_k):
        cluster_data = df[df['cluster'] == i]
        avg_income = cluster_data['annual_income'].mean()
        avg_spend = cluster_data['spending_score'].mean()
        print(f"   â€¢ Cluster {i}: Venit={avg_income:.1f}k, Spending={avg_spend:.1f}")


# ==============================================================================
# PARTEA 3: K-NEAREST NEIGHBORS
# ==============================================================================

def demo_knn():
    """
    ComparÄƒ K-NN implementat Ã®n C cu sklearn.
    
    sklearn KNN are:
    - Ball Tree È™i KD Tree pentru cÄƒutare rapidÄƒ
    - Distance weighting
    - Suport pentru diverse metrici
    """
    print("\n" + "="*70)
    print("PARTEA 3: K-NEAREST NEIGHBORS")
    print("="*70)
    
    # ÃncarcÄƒ datele Iris
    try:
        df = pd.read_csv('data/iris.csv', comment='#')
    except FileNotFoundError:
        print("EROARE: FiÈ™ierul data/iris.csv nu a fost gÄƒsit!")
        return
    
    # PregÄƒteÈ™te datele
    X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
    y = df['species'].values
    
    # Normalizare
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # TesteazÄƒ diferite valori de k
    print("\nğŸ“Š AcurateÈ›e pentru diferite valori de k:")
    print("-" * 40)
    
    for k in [1, 3, 5, 7, 9]:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train)
        acc = knn.score(X_test, y_test)
        print(f"   k={k}: AcurateÈ›e = {acc:.4f} ({acc*100:.1f}%)")
    
    # Model final cu k=5
    optimal_k = 5
    start_time = time.time()
    knn = KNeighborsClassifier(n_neighbors=optimal_k, algorithm='brute')
    knn.fit(X_train, y_train)
    sklearn_time = time.time() - start_time
    
    y_pred = knn.predict(X_test)
    
    print(f"\nğŸ“Š Rezultate sklearn KNeighborsClassifier (k={optimal_k}):")
    print(f"   â€¢ Timp antrenare: {sklearn_time*1000:.3f} ms (lazy learner)")
    print(f"   â€¢ AcurateÈ›e (test): {accuracy_score(y_test, y_pred):.4f}")
    
    print(f"\n   Classification Report:")
    print(classification_report(y_test, y_pred, target_names=['setosa', 'versicolor', 'virginica']))
    
    print("ğŸ’¡ NotÄƒ: KNN este un 'lazy learner' - nu are fazÄƒ de antrenare propriu-zisÄƒ")


# ==============================================================================
# PARTEA 4: PERCEPTRON
# ==============================================================================

def demo_perceptron():
    """
    ComparÄƒ Perceptron implementat Ã®n C cu sklearn.
    
    DemonstreazÄƒ È™i limitarea: XOR nu poate fi Ã®nvÄƒÈ›at!
    """
    print("\n" + "="*70)
    print("PARTEA 4: PERCEPTRON")
    print("="*70)
    
    # === Test 1: Date liniar separabile (Setosa vs rest) ===
    print("\n--- Test 1: Clasificare binarÄƒ (Setosa vs Non-Setosa) ---")
    
    try:
        df = pd.read_csv('data/iris.csv', comment='#')
    except FileNotFoundError:
        print("EROARE: FiÈ™ierul data/iris.csv nu a fost gÄƒsit!")
        return
    
    X = df[['petal_length', 'petal_width']].values
    y = (df['species'] == 'setosa').astype(int).values
    
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    start_time = time.time()
    perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
    perceptron.fit(X_train, y_train)
    sklearn_time = time.time() - start_time
    
    y_pred = perceptron.predict(X_test)
    
    print(f"\nğŸ“Š Rezultate sklearn Perceptron:")
    print(f"   â€¢ Timp antrenare: {sklearn_time*1000:.3f} ms")
    print(f"   â€¢ IteraÈ›ii: {perceptron.n_iter_}")
    print(f"   â€¢ AcurateÈ›e (test): {accuracy_score(y_test, y_pred):.4f}")
    print(f"\n   Weights: w1={perceptron.coef_[0][0]:.4f}, w2={perceptron.coef_[0][1]:.4f}")
    print(f"   Bias: {perceptron.intercept_[0]:.4f}")
    
    # === Test 2: XOR - trebuie sÄƒ eÈ™ueze! ===
    print("\n--- Test 2: Problema XOR (trebuie sÄƒ EÈ˜UEZE) ---")
    
    # Date XOR pure
    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_xor = np.array([0, 1, 1, 0])
    
    perceptron_xor = Perceptron(max_iter=1000, tol=None, random_state=42)
    perceptron_xor.fit(X_xor, y_xor)
    y_pred_xor = perceptron_xor.predict(X_xor)
    
    print(f"\nğŸ“Š Perceptron pe XOR:")
    print(f"   â€¢ AcurateÈ›e: {accuracy_score(y_xor, y_pred_xor):.4f} (50% = random)")
    print(f"   â€¢ PredicÈ›ii: {y_pred_xor} (aÈ™teptat: [0 1 1 0])")
    print(f"\n   âš ï¸  CONCLUZIE: Perceptronul NU poate Ã®nvÄƒÈ›a XOR!")
    print("   â¤ XOR nu este liniar separabil")
    print("   â¤ NecesitÄƒ reÈ›ea cu cel puÈ›in un strat ascuns (MLP)")


# ==============================================================================
# PARTEA 5: COMPARAÈšIE DE TIMP
# ==============================================================================

def benchmark_comparison():
    """
    ComparÄƒ timpii de execuÈ›ie Ã®ntre implementarea C È™i sklearn.
    """
    print("\n" + "="*70)
    print("PARTEA 5: BENCHMARK COMPARATIV")
    print("="*70)
    
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Algoritm        â”‚  sklearn (ms)â”‚   C (ms)     â”‚   NotÄƒ      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Linear Regression  â”‚    ~0.5      â”‚   ~5-10      â”‚ C: iterativ â”‚
    â”‚ K-Means (k=4)      â”‚    ~2-5      â”‚   ~10-20     â”‚ C: single   â”‚
    â”‚ K-NN (k=5)         â”‚    ~0.1      â”‚   ~0.5-1     â”‚ lazy learn  â”‚
    â”‚ Perceptron         â”‚    ~1-2      â”‚   ~5-10      â”‚ convergence â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ObservaÈ›ii:
    â€¢ sklearn este optimizat Ã®n Cython/C sub capotÄƒ
    â€¢ sklearn foloseÈ™te BLAS/LAPACK pentru algebra liniarÄƒ
    â€¢ ImplementÄƒrile noastre C sunt DIDACTICE, nu pentru producÈ›ie
    â€¢ Valoarea educaÈ›ionalÄƒ: Ã®nÈ›elegem algoritmii Ã®n profunzime
    """)


# ==============================================================================
# MAIN
# ==============================================================================

def main():
    """
    RuleazÄƒ toate demonstraÈ›iile.
    """
    print("â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*15 + "COMPARAÈšIE: C vs scikit-learn" + " "*24 + "â•‘")
    print("â•‘" + " "*15 + "SÄƒptÄƒmÃ¢na 18 - ML Fundamentals" + " "*23 + "â•‘")
    print("â•š" + "â•"*68 + "â•")
    
    demo_linear_regression()
    demo_kmeans()
    demo_knn()
    demo_perceptron()
    benchmark_comparison()
    
    print("\n" + "="*70)
    print("âœ… Toate comparaÈ›iile finalizate!")
    print("="*70)


if __name__ == "__main__":
    main()

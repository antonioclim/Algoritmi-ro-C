# ğŸ“ Teme SÄƒptÄƒmÃ¢na 18: Fundamente Machine Learning Ã®n C

## InformaÈ›ii Generale

| Proprietate | Valoare |
|------------|---------|
| **Deadline** | SÄƒptÄƒmÃ¢na 19, ora laboratorului |
| **Punctaj maxim** | 100 puncte |
| **Bonus posibil** | +20 puncte (pentru Extended) |
| **Limbaj** | C11 standard |
| **RestricÈ›ii** | FÄƒrÄƒ biblioteci externe ML |

---

## ğŸ“‹ Tema 1: Ridge Regression (Regularizare L2) - 50 puncte

### Context Teoretic

**Regresia Ridge** (Tikhonov regularization) adaugÄƒ un termen de penalizare
la funcÈ›ia de cost pentru a preveni overfitting-ul:

```
J(w) = MSE(w) + Î»Â·||w||Â²
     = (1/n)Â·Î£(y_i - Å·_i)Â² + Î»Â·Î£w_jÂ²
```

Unde:
- `MSE(w)` = eroarea medie pÄƒtraticÄƒ (ca la regresie obiÈ™nuitÄƒ)
- `Î»` = parametrul de regularizare (hyperparametru)
- `||w||Â²` = suma pÄƒtratelor weight-urilor

**IntuiÈ›ie**: Penalizarea forÈ›eazÄƒ weight-urile sÄƒ rÄƒmÃ¢nÄƒ mici, prevenind
modelul sÄƒ se bazeze prea mult pe o singurÄƒ caracteristicÄƒ.

### Gradientul cu Regularizare

Gradientul devine:

```
âˆ‚J/âˆ‚w_j = (2/n)Â·Î£(Å·_i - y_i)Â·x_ij + 2Â·Î»Â·w_j
```

**Update rule** (Gradient Descent):
```
w_j = w_j - Î±Â·[(2/n)Â·Î£(errorÂ·x_ij) + 2Â·Î»Â·w_j]
    = w_jÂ·(1 - 2Â·Î±Â·Î») - Î±Â·(2/n)Â·Î£(errorÂ·x_ij)
```

### CerinÈ›e de Implementare

CreaÈ›i fiÈ™ierul `homework1.c` cu urmÄƒtoarele:

```c
// ============================================================
// STRUCTURA RIDGE REGRESSION
// ============================================================
typedef struct {
    double *weights;      // Vector de weight-uri
    double bias;          // Bias term
    int n_features;       // NumÄƒr de caracteristici
    double lambda;        // Parametru de regularizare
    double learning_rate; // Rata de Ã®nvÄƒÈ›are
    int max_iters;        // NumÄƒr maxim de iteraÈ›ii
    double tolerance;     // ToleranÈ›Äƒ pentru convergenÈ›Äƒ
} RidgeRegression;

// ============================================================
// FUNCÈšII DE IMPLEMENTAT
// ============================================================

/**
 * CreeazÄƒ È™i iniÈ›ializeazÄƒ un model Ridge Regression.
 * 
 * @param n_features  NumÄƒrul de caracteristici din date
 * @param lambda      Parametrul de regularizare (0.01 - 10.0 tipic)
 * @param lr          Learning rate (0.001 - 0.1 tipic)
 * @param max_iters   NumÄƒr maxim de iteraÈ›ii (1000 - 10000)
 * @param tol         ToleranÈ›Äƒ pentru early stopping (1e-6)
 * @return            Pointer la structurÄƒ alocatÄƒ
 */
RidgeRegression *ridge_create(int n_features, double lambda, 
                              double lr, int max_iters, double tol);

/**
 * ElibereazÄƒ memoria pentru model.
 */
void ridge_free(RidgeRegression *model);

/**
 * CalculeazÄƒ funcÈ›ia de cost cu regularizare.
 * 
 * J(w) = (1/n)Â·Î£(y - Å·)Â² + Î»Â·Î£wÂ²
 * 
 * @return Valoarea funcÈ›iei de cost
 */
double ridge_cost(RidgeRegression *model, double **X, double *y, int n);

/**
 * AntreneazÄƒ modelul folosind Gradient Descent cu regularizare L2.
 * 
 * @param X      Matricea de caracteristici [n_samples][n_features]
 * @param y      Vectorul de target-uri
 * @param n      NumÄƒrul de exemple
 * @return       NumÄƒrul de iteraÈ›ii pÃ¢nÄƒ la convergenÈ›Äƒ
 */
int ridge_fit(RidgeRegression *model, double **X, double *y, int n);

/**
 * Prezice valorile pentru date noi.
 * 
 * @param X      Matricea de caracteristici
 * @param n      NumÄƒrul de exemple
 * @param y_pred Buffer pentru predicÈ›ii (pre-alocat)
 */
void ridge_predict(RidgeRegression *model, double **X, int n, double *y_pred);

/**
 * CalculeazÄƒ RÂ² score.
 */
double ridge_r2_score(double *y_true, double *y_pred, int n);
```

### Testare

FolosiÈ›i `data/housing_simple.csv` È™i comparaÈ›i:
1. Ridge cu Î»=0 (echivalent cu Linear Regression obiÈ™nuitÄƒ)
2. Ridge cu Î»=0.1, Î»=1.0, Î»=10.0
3. ObservaÈ›i cum weight-urile devin mai mici cu Î» mai mare

### Criterii de Evaluare (50p)

| Criteriu | Puncte |
|----------|--------|
| Implementare corectÄƒ `ridge_cost` | 10p |
| Implementare corectÄƒ `ridge_fit` cu gradient | 20p |
| ConvergenÈ›Äƒ È™i early stopping | 5p |
| PredicÈ›ii corecte | 10p |
| ComparaÈ›ie diferite Î» Ã®n main() | 5p |

---

## ğŸ“‹ Tema 2: Weighted K-NN - 50 puncte

### Context Teoretic

**Weighted K-NN** Ã®mbunÄƒtÄƒÈ›eÈ™te K-NN clasic prin acordarea de ponderi
vecinilor Ã®n funcÈ›ie de distanÈ›a lor:

```
Vot ponderat: P(clasÄƒ c) = Î£ w_i Â· I(y_i = c) / Î£ w_i
```

Unde `w_i` este ponderea vecinului `i`, calculatÄƒ ca:

1. **Inversul distanÈ›ei**: `w_i = 1 / d_i`
2. **Gaussian kernel**: `w_i = exp(-d_iÂ² / (2ÏƒÂ²))`
3. **Linear decay**: `w_i = (k - rank_i + 1) / k`

**Avantaj**: Vecinii mai apropiaÈ›i au influenÈ›Äƒ mai mare Ã®n decizie.

### CerinÈ›e de Implementare

CreaÈ›i fiÈ™ierul `homework2.c` cu urmÄƒtoarele:

```c
// ============================================================
// ENUMÄ‚RI È˜I STRUCTURI
// ============================================================
typedef enum {
    WEIGHT_UNIFORM,        // KNN clasic (toate ponderile = 1)
    WEIGHT_INVERSE_DIST,   // w = 1/d
    WEIGHT_GAUSSIAN,       // w = exp(-dÂ²/2ÏƒÂ²)
    WEIGHT_LINEAR_DECAY    // w = (k - rank + 1) / k
} WeightScheme;

typedef struct {
    int k;                 // NumÄƒr de vecini
    WeightScheme scheme;   // Schema de ponderare
    double sigma;          // Parametru pentru Gaussian (dacÄƒ e cazul)
    
    // Date de antrenare (stocare lazy)
    double **X_train;
    int *y_train;
    int n_train;
    int n_features;
    int n_classes;
} WeightedKNN;

// ============================================================
// FUNCÈšII DE IMPLEMENTAT
// ============================================================

/**
 * CreeazÄƒ clasificator Weighted K-NN.
 */
WeightedKNN *wknn_create(int k, WeightScheme scheme, double sigma);

/**
 * ElibereazÄƒ memoria.
 */
void wknn_free(WeightedKNN *model);

/**
 * "AntreneazÄƒ" modelul (stocare date pentru lazy learning).
 * 
 * @param X        Matricea de caracteristici
 * @param y        Etichetele (0, 1, 2, ...)
 * @param n        NumÄƒrul de exemple
 * @param n_feat   NumÄƒrul de caracteristici
 * @param n_class  NumÄƒrul de clase
 */
void wknn_fit(WeightedKNN *model, double **X, int *y, 
              int n, int n_feat, int n_class);

/**
 * CalculeazÄƒ ponderea pentru un vecin.
 * 
 * @param distance  DistanÈ›a pÃ¢nÄƒ la vecin
 * @param rank      PoziÈ›ia Ã®n ordinea distanÈ›elor (1 = cel mai apropiat)
 * @return          Ponderea calculatÄƒ
 */
double wknn_compute_weight(WeightedKNN *model, double distance, int rank);

/**
 * Prezice clasa pentru un singur exemplu.
 * 
 * @param x        Vectorul de caracteristici
 * @return         Clasa prezisÄƒ
 */
int wknn_predict_one(WeightedKNN *model, double *x);

/**
 * Prezice clasele pentru mai multe exemple.
 */
void wknn_predict(WeightedKNN *model, double **X, int n, int *y_pred);

/**
 * CalculeazÄƒ acurateÈ›ea.
 */
double wknn_accuracy(int *y_true, int *y_pred, int n);

/**
 * ComparÄƒ diferite scheme de ponderare.
 */
void wknn_compare_schemes(double **X_train, int *y_train, int n_train,
                          double **X_test, int *y_test, int n_test,
                          int n_features, int n_classes);
```

### Exemplu de Vot Ponderat

```
Exemplu cu k=5, clasÄƒ = {0, 1}:

Vecin 1: clasÄƒ=1, distanÈ›Äƒ=0.5 â†’ w=1/0.5=2.0
Vecin 2: clasÄƒ=0, distanÈ›Äƒ=1.0 â†’ w=1/1.0=1.0
Vecin 3: clasÄƒ=1, distanÈ›Äƒ=1.2 â†’ w=1/1.2=0.83
Vecin 4: clasÄƒ=1, distanÈ›Äƒ=2.0 â†’ w=1/2.0=0.5
Vecin 5: clasÄƒ=0, distanÈ›Äƒ=3.0 â†’ w=1/3.0=0.33

Scor clasÄƒ 0: 1.0 + 0.33 = 1.33
Scor clasÄƒ 1: 2.0 + 0.83 + 0.5 = 3.33

â†’ PredicÈ›ie: clasÄƒ 1 (scor mai mare)

Cu KNN clasic (uniform): 3 voturi pentru 1, 2 pentru 0 â†’ tot clasÄƒ 1
Dar ponderile dau mai multÄƒ Ã®ncredere!
```

### Testare

FolosiÈ›i `data/iris.csv` È™i comparaÈ›i:
1. KNN uniform (k=5)
2. Weighted KNN inverse distance (k=5)
3. Weighted KNN Gaussian (k=5, Ïƒ=0.5)
4. TestaÈ›i cu k=3, k=7, k=11

### Criterii de Evaluare (50p)

| Criteriu | Puncte |
|----------|--------|
| Implementare `wknn_compute_weight` pentru toate schemele | 15p |
| Implementare corectÄƒ `wknn_predict_one` | 15p |
| Testare pe Iris dataset | 10p |
| ComparaÈ›ie scheme + analizÄƒ rezultate | 10p |

---

## ğŸ“¤ InstrucÈ›iuni de Predare

### Format ArhivÄƒ

```
homework_week18_NUME_PRENUME.zip
â”œâ”€â”€ homework1.c          # Ridge Regression
â”œâ”€â”€ homework2.c          # Weighted K-NN
â”œâ”€â”€ Makefile             # Pentru compilare
â”œâ”€â”€ README.md            # DocumentaÈ›ie (opÈ›ional dar recomandat)
â””â”€â”€ results/
    â”œâ”€â”€ ridge_comparison.txt    # Output comparaÈ›ie Î»
    â””â”€â”€ wknn_comparison.txt     # Output comparaÈ›ie scheme
```

### Makefile Minimal

```makefile
CC = gcc
CFLAGS = -Wall -Wextra -std=c11 -O2
LDFLAGS = -lm

all: homework1 homework2

homework1: homework1.c
	$(CC) $(CFLAGS) -o $@ $< $(LDFLAGS)

homework2: homework2.c
	$(CC) $(CFLAGS) -o $@ $< $(LDFLAGS)

clean:
	rm -f homework1 homework2

.PHONY: all clean
```

### Criterii de Admitere

- [ ] Codul compileazÄƒ fÄƒrÄƒ erori cu `gcc -Wall -Wextra`
- [ ] Nu existÄƒ memory leaks (verificat cu Valgrind)
- [ ] FuncÈ›ioneazÄƒ pe datele furnizate
- [ ] Rezultatele sunt rezonabile (RÂ² > 0.8 pentru Ridge, AcurateÈ›e > 90% pentru KNN)

---

## ğŸ”— Resurse Utile

1. **Ridge Regression**: [Elements of Statistical Learning, Cap. 3.4](https://hastie.su.domains/ElemStatLearn/)
2. **Weighted K-NN**: [Dudani (1976) - The Distance-Weighted k-Nearest-Neighbor Rule](https://ieeexplore.ieee.org/document/4309452)
3. **Gradient Descent**: [Stanford CS229 Notes](https://cs229.stanford.edu/notes2022fall/main_notes.pdf)

---

*Succes la implementare! ÃntrebÄƒrile se adreseazÄƒ Ã®n cadrul laboratorului sau pe forum.*

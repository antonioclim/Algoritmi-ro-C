# ğŸš€ Teme Extended SÄƒptÄƒmÃ¢na 18: ProvocÄƒri Avansate ML

## InformaÈ›ii Generale

| Proprietate | Valoare |
|------------|---------|
| **Tip** | BONUS (opÈ›ional) |
| **Punctaj maxim** | +20 puncte la nota finalÄƒ |
| **Deadline** | SÄƒptÄƒmÃ¢na 20 |
| **Dificultate** | â­â­â­ Avansat |

> **NotÄƒ**: Aceste provocÄƒri sunt pentru studenÈ›ii care doresc sÄƒ aprofundeze 
> conceptele ML. Nu sunt obligatorii pentru promovare.

---

## ğŸ† Provocare 1: Logistic Regression (4 puncte)

### Descriere

ImplementaÈ›i **Regresia LogisticÄƒ** pentru clasificare binarÄƒ, folosind
funcÈ›ia sigmoid È™i binary cross-entropy loss.

### Fundamente Teoretice

**FuncÈ›ia Sigmoid**:
```
Ïƒ(z) = 1 / (1 + e^(-z))

ProprietÄƒÈ›i:
- Ïƒ(0) = 0.5
- Ïƒ(z) â†’ 1 cÃ¢nd z â†’ +âˆ
- Ïƒ(z) â†’ 0 cÃ¢nd z â†’ -âˆ
- Ïƒ'(z) = Ïƒ(z) Â· (1 - Ïƒ(z))
```

**Modelul**:
```
P(y=1|x) = Ïƒ(wÂ·x + b)
```

**Binary Cross-Entropy Loss**:
```
J(w) = -(1/n) Â· Î£ [y_iÂ·log(Å·_i) + (1-y_i)Â·log(1-Å·_i)]
```

**Gradient**:
```
âˆ‚J/âˆ‚w = (1/n) Â· Î£ (Å·_i - y_i) Â· x_i
âˆ‚J/âˆ‚b = (1/n) Â· Î£ (Å·_i - y_i)
```

### CerinÈ›e

```c
typedef struct {
    double *weights;
    double bias;
    int n_features;
    double learning_rate;
    int max_iters;
} LogisticRegression;

// FuncÈ›ii de implementat
double sigmoid(double z);
LogisticRegression *logistic_create(int n_features, double lr, int max_iters);
void logistic_free(LogisticRegression *model);
double logistic_bce_loss(LogisticRegression *model, double **X, int *y, int n);
int logistic_fit(LogisticRegression *model, double **X, int *y, int n);
double logistic_predict_proba(LogisticRegression *model, double *x);
int logistic_predict(LogisticRegression *model, double *x);
```

### Test

Clasificare Iris: Setosa vs non-Setosa (folosind doar petal features).

---

## ğŸ† Provocare 2: Mini-Batch Gradient Descent (4 puncte)

### Descriere

ImplementaÈ›i **Mini-Batch Gradient Descent** pentru Linear Regression,
cu suport pentru batch_size configurabil.

### Context

ExistÄƒ trei variante de Gradient Descent:

| Tip | Batch Size | Avantaje | Dezavantaje |
|-----|------------|----------|-------------|
| Batch GD | n (toate) | ConvergenÈ›Äƒ stabilÄƒ | Lent pe date mari |
| Stochastic GD | 1 | Rapid, escape local minima | Zgomot, instabil |
| Mini-Batch GD | 16-256 | BalanÈ› vitezÄƒ/stabilitate | Hyperparametru extra |

### CerinÈ›e

```c
typedef struct {
    double *weights;
    double bias;
    int n_features;
    double learning_rate;
    int max_epochs;
    int batch_size;
    int shuffle;  // 1 = shuffle data fiecare epoch
} MiniBatchGD;

// FuncÈ›ii de implementat
MiniBatchGD *mbgd_create(int n_features, double lr, int epochs, int batch_size);
void mbgd_free(MiniBatchGD *model);

// Shuffle indices (Fisher-Yates)
void shuffle_indices(int *indices, int n);

// Un pas de gradient pe un mini-batch
void mbgd_update_batch(MiniBatchGD *model, double **X, double *y, 
                       int *indices, int start, int end);

// Antrenare completÄƒ
int mbgd_fit(MiniBatchGD *model, double **X, double *y, int n);

// PredicÈ›ii
void mbgd_predict(MiniBatchGD *model, double **X, int n, double *y_pred);
```

### Test

ComparaÈ›i convergenÈ›a pentru batch_size = 1, 8, 32, n pe housing data.
GeneraÈ›i grafic ASCII cu loss vs epoch pentru fiecare variantÄƒ.

---

## ğŸ† Provocare 3: PCA (Principal Component Analysis) (4 puncte)

### Descriere

ImplementaÈ›i **PCA** pentru reducerea dimensionalitÄƒÈ›ii folosind
Power Iteration pentru calculul vectorilor proprii.

### Fundamente

**PaÈ™ii PCA**:
1. Centrare date: X_centered = X - mean(X)
2. Calcul matrice de covarianÈ›Äƒ: C = (1/n) Â· X^T Â· X
3. Calcul vectori proprii (eigenvectors) ai lui C
4. ProiecÈ›ie pe primii k vectori proprii

**Power Iteration** (pentru dominant eigenvector):
```
vâ‚€ = random vector
repeat:
    v_{k+1} = A Â· v_k
    v_{k+1} = v_{k+1} / ||v_{k+1}||
until convergence
Î» = v^T Â· A Â· v  (eigenvalue)
```

### CerinÈ›e

```c
typedef struct {
    int n_components;     // NumÄƒr de componente de pÄƒstrat
    double **components;  // Vectori proprii (k x n_features)
    double *eigenvalues;  // Valori proprii
    double *mean;         // Media pe fiecare coloanÄƒ
    double explained_var; // VarianÈ›a explicatÄƒ (%)
    int n_features;
} PCA;

// FuncÈ›ii de implementat
PCA *pca_create(int n_components);
void pca_free(PCA *pca);

// Centrare date
void pca_center(double **X, int n, int d, double *mean);

// Matrice covarianÈ›Äƒ
void pca_covariance(double **X_centered, int n, int d, double **cov);

// Power iteration pentru un eigenvector
void power_iteration(double **A, int n, double *eigenvector, 
                     double *eigenvalue, int max_iters);

// Fit: calculeazÄƒ componentele principale
void pca_fit(PCA *pca, double **X, int n, int d);

// Transform: proiecteazÄƒ date pe componentele principale
void pca_transform(PCA *pca, double **X, int n, double **X_reduced);

// Inverse transform (reconstrucÈ›ie)
void pca_inverse_transform(PCA *pca, double **X_reduced, int n, double **X_reconstructed);
```

### Test

ReduceÈ›i Iris de la 4D la 2D È™i vizualizaÈ›i (ASCII scatter plot).
CalculaÈ›i reconstruction error pentru k=1, 2, 3, 4.

---

## ğŸ† Provocare 4: Decision Stump (4 puncte)

### Descriere

ImplementaÈ›i **Decision Stump** - un arbore de decizie cu o singurÄƒ
ramificare. Este building block-ul pentru AdaBoost.

### Concept

Un Decision Stump gÄƒseÈ™te:
- Cea mai bunÄƒ caracteristicÄƒ pe care sÄƒ se separe
- Cel mai bun threshold pentru separare
- OptimizeazÄƒ un criteriu (Gini impurity sau Information Gain)

```
         [feature_j â‰¤ threshold?]
              /           \
           YES             NO
            |               |
       [clasÄƒ_stÃ¢nga]  [clasÄƒ_dreapta]
```

### Gini Impurity

```
Gini(S) = 1 - Î£ p_iÂ²

Unde p_i = proporÈ›ia clasei i Ã®n setul S

Gini split = (n_left/n) Â· Gini(S_left) + (n_right/n) Â· Gini(S_right)
```

### CerinÈ›e

```c
typedef struct {
    int feature_index;     // Pe ce caracteristicÄƒ se Ã®mparte
    double threshold;      // Pragul de Ã®mpÄƒrÈ›ire
    int class_left;        // Clasa pentru <= threshold
    int class_right;       // Clasa pentru > threshold
    double gini;           // Gini impurity al split-ului
} DecisionStump;

// FuncÈ›ii de implementat
DecisionStump *stump_create();
void stump_free(DecisionStump *stump);

// Calcul Gini impurity
double gini_impurity(int *labels, int n, int n_classes);

// EvalueazÄƒ un posibil split
double evaluate_split(double **X, int *y, int n, int n_classes,
                      int feature_idx, double threshold);

// GÄƒseÈ™te cel mai bun split
void stump_fit(DecisionStump *stump, double **X, int *y, 
               int n, int n_features, int n_classes);

// PredicÈ›ie
int stump_predict_one(DecisionStump *stump, double *x);
void stump_predict(DecisionStump *stump, double **X, int n, int *y_pred);

// Print stump
void stump_print(DecisionStump *stump);
```

### Test

AntrenaÈ›i pe Iris (toate cele 3 clase) È™i afiÈ™aÈ›i regula gÄƒsitÄƒ.
ComparaÈ›i cu K-NN È™i Perceptron pe aceleaÈ™i date.

---

## ğŸ† Provocare 5: Naive Bayes (4 puncte)

### Descriere

ImplementaÈ›i **Gaussian Naive Bayes** pentru clasificare, bazat pe
teorema lui Bayes cu asumpÈ›ia de independenÈ›Äƒ.

### Fundamente

**Teorema lui Bayes**:
```
P(c|x) = P(x|c) Â· P(c) / P(x)
```

**Naive Bayes asumpÈ›ie**: caracteristicile sunt independente condiÈ›ionat
```
P(x|c) = Î  P(x_i|c)
```

**Gaussian Naive Bayes**: fiecare P(x_i|c) este o distribuÈ›ie normalÄƒ
```
P(x_i|c) = (1/âˆš(2Ï€Â·ÏƒÂ²_ic)) Â· exp(-(x_i - Î¼_ic)Â² / (2Â·ÏƒÂ²_ic))
```

### CerinÈ›e

```c
typedef struct {
    int n_classes;
    int n_features;
    double *class_priors;    // P(c) pentru fiecare clasÄƒ
    double **means;          // Î¼_ic: mean[class][feature]
    double **variances;      // ÏƒÂ²_ic: variance[class][feature]
    int *class_counts;       // NumÄƒr exemple per clasÄƒ
} GaussianNB;

// FuncÈ›ii de implementat
GaussianNB *gnb_create(int n_classes, int n_features);
void gnb_free(GaussianNB *model);

// Calcul probabilitate GaussianÄƒ
double gaussian_pdf(double x, double mean, double variance);

// Antrenare: calculeazÄƒ P(c), Î¼_ic, ÏƒÂ²_ic
void gnb_fit(GaussianNB *model, double **X, int *y, int n);

// Log-likelihood pentru stabilitate numericÄƒ
double gnb_log_likelihood(GaussianNB *model, double *x, int class_idx);

// PredicÈ›ie
int gnb_predict_one(GaussianNB *model, double *x);
void gnb_predict(GaussianNB *model, double **X, int n, int *y_pred);

// ProbabilitÄƒÈ›i per clasÄƒ
void gnb_predict_proba(GaussianNB *model, double *x, double *proba);
```

### Test

Clasificare Iris cu toate 3 clase. AfiÈ™aÈ›i:
- P(c) pentru fiecare clasÄƒ
- Î¼ È™i Ïƒ pentru fiecare (clasÄƒ, caracteristicÄƒ)
- Matrice de confuzie finalÄƒ

---

## ğŸ“Š Sistem de Punctare Bonus

| Provocare | Puncte | Total cumulat |
|-----------|--------|---------------|
| 1. Logistic Regression | +4 | +4 |
| 2. Mini-Batch GD | +4 | +8 |
| 3. PCA | +4 | +12 |
| 4. Decision Stump | +4 | +16 |
| 5. Naive Bayes | +4 | +20 |

**Maximum +20 puncte bonus** care se adaugÄƒ la nota de laborator.

---

## ğŸ“¤ Format Predare Extended

```
homework_extended_week18_NUME_PRENUME.zip
â”œâ”€â”€ challenge1_logistic.c
â”œâ”€â”€ challenge2_minibatch.c
â”œâ”€â”€ challenge3_pca.c
â”œâ”€â”€ challenge4_stump.c
â”œâ”€â”€ challenge5_naivebayes.c
â”œâ”€â”€ Makefile
â””â”€â”€ results/
    â””â”€â”€ [output files pentru fiecare challenge]
```

---

## ğŸ’¡ Sfaturi

1. **ÃncepeÈ›i cu ce vÄƒ pare mai uÈ™or** - Naive Bayes È™i Decision Stump sunt
   relativ directe de implementat.

2. **FolosiÈ›i log-probabilitÄƒÈ›i** pentru stabilitate numericÄƒ Ã®n Naive Bayes
   È™i Logistic Regression.

3. **TestaÈ›i pe date mici** Ã®nainte de datasetul complet.

4. **ComparaÈ›i cu sklearn** pentru validare (vezi `python_comparison/`).

5. **Nu uitaÈ›i de edge cases**: division by zero, variance zero, etc.

---

*Aceste provocÄƒri vÄƒ vor pregÄƒti pentru cursuri avansate de ML È™i 
interviuri tehnice. Succes!*
